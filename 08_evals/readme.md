# Session 8: Evals You’ll Actually Use (No‑Code)

## Why
Quality loops make agents reliable. We’ll design a small eval dataset, run trace grading, and use prompt optimization to improve outcomes.

## Objectives
- Create a 20‑case eval dataset from real prompts/tickets
- Use trace grading to identify brittle steps
- Apply prompt optimization and re‑run to compare
- Design and apply graders to measure your agent performence and iteraively improve it.

## Lab
1. Build dataset: label intents, expected behaviors, acceptance notes
2. Run initial eval; bookmark failing traces
3. Use grader suggestions to refine instructions
4. Re‑run; capture deltas (pass rate, tokens, latency)

## Deliverable
- Eval report with before/after metrics and 2 documented prompt changes

## Resources
- OpenAI Evals (trace grading, prompt optimization)